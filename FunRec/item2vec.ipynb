{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b70602f",
   "metadata": {},
   "source": [
    "手动实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c8717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class Item2Vec:\n",
    "    def __init__(self, item_size, embedding_dim, neg_samples=5):\n",
    "        \"\"\"\n",
    "        初始化 Item2Vec 模型。\n",
    "\n",
    "        参数:\n",
    "        - item_size: 物品表的大小（词汇表大小）。\n",
    "        - embedding_dim: 词向量的维度。\n",
    "        - neg_samples: 负样本的数量。\n",
    "        \"\"\"\n",
    "        self.item_size = item_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.neg_samples = neg_samples\n",
    "        \n",
    "        # 初始化中心物品向量和上下文物品向量\n",
    "        self.center_embeddings = np.random.randn(item_size, embedding_dim) * 0.01\n",
    "        self.context_embeddings = np.random.randn(item_size, embedding_dim) * 0.01\n",
    "        \n",
    "    def negative_sampling_loss(self, center_idx, context_idx, neg_sample_indices):\n",
    "        \"\"\"\n",
    "        计算负采样的损失函数。\n",
    "\n",
    "        参数:\n",
    "        - center_idx: 中心物品的索引。\n",
    "        - context_idx: 目标上下文物品的索引。\n",
    "        - neg_sample_indices: 负样本物品的索引列表。\n",
    "\n",
    "        返回:\n",
    "        - loss: 负采样的损失值。\n",
    "        - grad_center: 中心物品向量的梯度。\n",
    "        - grad_context: 目标上下文物品向量的梯度。\n",
    "        - grad_neg: 负样本物品向量的梯度。\n",
    "        \"\"\"\n",
    "        # 获取向量\n",
    "        v_c = self.center_embeddings[center_idx]  # 中心物品向量\n",
    "        u_o = self.context_embeddings[context_idx]  # 目标上下文物品向量\n",
    "        u_neg = self.context_embeddings[neg_sample_indices]  # 负样本物品向量\n",
    "        \n",
    "        # 正样本项\n",
    "        pos_score = np.dot(u_o, v_c)\n",
    "        pos_loss = -np.log(sigmoid(pos_score))\n",
    "        \n",
    "        # 负样本项\n",
    "        neg_scores = np.dot(u_neg, v_c)\n",
    "        neg_loss = -np.sum(np.log(sigmoid(-neg_scores)))\n",
    "        \n",
    "        # 总损失\n",
    "        loss = pos_loss + neg_loss\n",
    "        \n",
    "        # 计算梯度\n",
    "        grad_pos = sigmoid(pos_score) - 1  # 正样本的梯度\n",
    "        grad_neg = sigmoid(neg_scores)    # 负样本的梯度\n",
    "        \n",
    "        # 更新梯度\n",
    "        grad_center = grad_pos * u_o + np.sum(grad_neg.reshape(-1, 1) * u_neg, axis=0)\n",
    "        grad_context = grad_pos * v_c\n",
    "        grad_neg = grad_neg.reshape(-1, 1) * v_c\n",
    "        \n",
    "        return loss, grad_center, grad_context, grad_neg\n",
    "    \n",
    "    def train(self, item_pairs, epochs=10, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        训练 Item2Vec 模型。\n",
    "\n",
    "        参数:\n",
    "        - item_pairs: 物品对列表，每个物品对是一个元组 (center_idx, context_idx)。\n",
    "        - epochs: 训练轮数。\n",
    "        - learning_rate: 学习率。\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for center_idx, context_idx in item_pairs:\n",
    "                # 随机采样负样本\n",
    "                neg_sample_indices = np.random.choice(self.item_size, self.neg_samples, replace=False)\n",
    "                \n",
    "                # 计算损失和梯度\n",
    "                loss, grad_center, grad_context, grad_neg = self.negative_sampling_loss(\n",
    "                    center_idx, context_idx, neg_sample_indices\n",
    "                )\n",
    "                \n",
    "                # 更新中心物品向量\n",
    "                self.center_embeddings[center_idx] -= learning_rate * grad_center\n",
    "                \n",
    "                # 更新目标上下文物品向量\n",
    "                self.context_embeddings[context_idx] -= learning_rate * grad_context\n",
    "                \n",
    "                # 更新负样本物品向量\n",
    "                for i, idx in enumerate(neg_sample_indices):\n",
    "                    self.context_embeddings[idx] -= learning_rate * grad_neg[i]\n",
    "                \n",
    "                # 累加损失\n",
    "                total_loss += loss\n",
    "            \n",
    "            # 打印每轮的损失\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n",
    "    \n",
    "    def get_item_embeddings(self):\n",
    "        \"\"\"\n",
    "        获取物品的向量表示。\n",
    "\n",
    "        返回:\n",
    "        - item_embeddings: 物品的向量表示，形状为 (item_size, embedding_dim)。\n",
    "        \"\"\"\n",
    "        return self.center_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92fff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.397745936735637\n",
      "Epoch 2, Loss: 10.397785586755447\n",
      "Epoch 3, Loss: 10.397789086255093\n",
      "Epoch 4, Loss: 10.397140498903765\n",
      "Epoch 5, Loss: 10.397580954268332\n",
      "Epoch 6, Loss: 10.397638121158959\n",
      "Epoch 7, Loss: 10.397695438841511\n",
      "Epoch 8, Loss: 10.3973008814061\n",
      "Epoch 9, Loss: 10.397416728367025\n",
      "Epoch 10, Loss: 10.3970063986563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00693019,  0.00060289, -0.02170743],\n",
       "       [ 0.01652888,  0.00412437,  0.0002566 ],\n",
       "       [-0.01116124, -0.01332822, -0.00472743],\n",
       "       [-0.01387696,  0.00951275, -0.00366075],\n",
       "       [-0.00297341,  0.00662084,  0.00080711]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设物品表的大小为 5\n",
    "item_size = 5\n",
    "\n",
    "# 假设物品对列表\n",
    "item_pairs = [\n",
    "    (0, 1),  # 物品 0 和物品 1 共现\n",
    "    (0, 2),  # 物品 0 和物品 2 共现\n",
    "    (1, 2),  # 物品 1 和物品 2 共现\n",
    "    (2, 3),  # 物品 2 和物品 3 共现\n",
    "    (3, 4)   # 物品 3 和物品 4 共现\n",
    "]\n",
    "model=Item2Vec(item_size,3,2)\n",
    "model.train(item_pairs, epochs=10, learning_rate=0.03)\n",
    "model.get_item_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596437e2",
   "metadata": {},
   "source": [
    "pytorch实现方式：\n",
    "- 使用 PyTorch 实现 Item2Vec 模型，并利用 PyTorch 的自动微分功能来计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24ae5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "class Item2Vec(nn.Module):\n",
    "    def __init__(self, item_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        初始化 Item2Vec 模型。\n",
    "\n",
    "        参数:\n",
    "        - item_size: 物品表的大小（词汇表大小）\n",
    "        - embedding_dim: 词向量的维度。\n",
    "        \"\"\"\n",
    "        super(Item2Vec, self).__init__()\n",
    "        self.item_size = item_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 定义中心物品和上下文物品的嵌入层\n",
    "        self.center_embeddings = nn.Embedding(item_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(item_size, embedding_dim)\n",
    "        \n",
    "    def forward(self, center_idx, context_idx, neg_sample_indices):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "\n",
    "        参数:\n",
    "        - center_idx: 中心物品的索引。\n",
    "        - context_idx: 目标上下文物品的索引。\n",
    "        - neg_sample_indices: 负样本物品的索引列表。\n",
    "\n",
    "        返回:\n",
    "        - loss: 负采样的损失值。\n",
    "        \"\"\"\n",
    "        # 获取中心物品和上下文物品的嵌入向量\n",
    "        v_c = self.center_embeddings(center_idx)  # 中心物品向量\n",
    "        u_o = self.context_embeddings(context_idx)  # 目标上下文物品向量\n",
    "        u_neg = self.context_embeddings(neg_sample_indices)  # 负样本物品向量\n",
    "        \n",
    "        # 正样本项\n",
    "        pos_score = torch.matmul(u_o, v_c.t())\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_score))\n",
    "        \n",
    "        # 负样本项\n",
    "        neg_scores = torch.matmul(u_neg, v_c.t())\n",
    "        neg_loss = -torch.sum(torch.log(torch.sigmoid(-neg_scores)))\n",
    "        \n",
    "        # 总损失\n",
    "        loss = pos_loss + neg_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf913ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 16.910038948059082\n",
      "Epoch 2, Loss: 11.8334242105484\n",
      "Epoch 3, Loss: 15.881965637207031\n",
      "Epoch 4, Loss: 13.787595987319946\n",
      "Epoch 5, Loss: 11.726338863372803\n",
      "Epoch 6, Loss: 13.863561511039734\n",
      "Epoch 7, Loss: 12.330938816070557\n",
      "Epoch 8, Loss: 10.64801561832428\n",
      "Epoch 9, Loss: 12.697281002998352\n",
      "Epoch 10, Loss: 13.352895379066467\n"
     ]
    }
   ],
   "source": [
    "# 假设物品表的大小为 5\n",
    "item_size = 5\n",
    "\n",
    "# 假设词向量维度为 3\n",
    "embedding_dim = 3\n",
    "\n",
    "# 初始化模型\n",
    "model = Item2Vec(item_size, embedding_dim)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 假设物品对列表\n",
    "item_pairs = [\n",
    "    (0, 1),  # 物品 0 和物品 1 共现\n",
    "    (0, 2),  # 物品 0 和物品 2 共现\n",
    "    (1, 2),  # 物品 1 和物品 2 共现\n",
    "    (2, 3),  # 物品 2 和物品 3 共现\n",
    "    (3, 4)   # 物品 3 和物品 4 共现\n",
    "]\n",
    "\n",
    "# 训练模型\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center_idx, context_idx in item_pairs:\n",
    "        # 随机采样负样本\n",
    "        neg_sample_indices = np.random.choice(item_size, 2, replace=False)\n",
    "        neg_sample_indices = torch.tensor(neg_sample_indices, dtype=torch.long)\n",
    "        \n",
    "        # 将索引转换为 PyTorch 张量\n",
    "        center_idx = torch.tensor(center_idx, dtype=torch.long)\n",
    "        context_idx = torch.tensor(context_idx, dtype=torch.long)\n",
    "        \n",
    "        # 清零梯度\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        loss = model(center_idx, context_idx, neg_sample_indices)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 累加损失\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 打印每轮的损失\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0abd6e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "物品的向量表示:\n",
      "tensor([[-0.8770,  1.8270, -0.0632],\n",
      "        [ 1.0472,  0.2641, -1.1805],\n",
      "        [-0.1065,  2.0799, -2.3516],\n",
      "        [ 0.3148, -0.5068, -1.4442],\n",
      "        [-0.1261,  0.0168,  2.0256]])\n"
     ]
    }
   ],
   "source": [
    "# 获取物品的向量表示\n",
    "item_embeddings = model.center_embeddings.weight.data\n",
    "print(\"物品的向量表示:\")\n",
    "print(item_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ad9a3",
   "metadata": {},
   "source": [
    "# 小知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa4045",
   "metadata": {},
   "source": [
    "1. 关于pytorch的backword\n",
    "\n",
    "PyTorch 采用动态计算图来跟踪所有涉及 `requires_grad=True` 的张量上的操作。在进行前向传播时，PyTorch 会记录每一个操作，构建一个计算图。当调用 `backward()` 方法时，PyTorch 会从损失值这个**标量**开始，沿着计算图反向传播，根据链式法则自动计算每个可学习参数（即 `requires_grad=True` 的张量）关于损失的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0da0a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.]) tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个需要求导的张量\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "# 定义一个简单的函数\n",
    "y = x ** 2\n",
    "# 计算损失（这里简单将 y 作为损失）\n",
    "loss = y\n",
    "# 调用 backward 方法计算梯度\n",
    "loss.backward()\n",
    "# 查看梯度\n",
    "print(x.data,x.grad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b1deb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
